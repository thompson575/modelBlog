---
title: "Methods: Nimble HMC (Second thoughts)"
author: "John Thompson"
date: "2022-09-27"
layout: post
tags:
- nimble
- HMC
- generalised linear models
output:
    html_document:
    keep_md: true
editor_options:
  chunk_output_type: console
---



<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>It is only yesterday that I wrote a post on <code>nimbleHMC</code> and already I have had second thoughts, some positive and some not so positive. In yesterday’s post, I hit a problem with dynamic indexing and I needed a work around based on dummy variables. Shortly, after posting I realised that there was a much simpler solution, which I’ll explain below. However, when I tested my new code, I realised that there were other issues that I had missed.</p>
<p>I wrote this current post as I tried different variations on yesterday’s HMC analysis, so it has ended as a kind of diary rather than a structured explanation. For that reason, it might be easier to follow if you see in advance the conclusions that I reached at the end of the process. Here they are,</p>
<ul>
<li>I remain optimistic that HMC will make nimble a more competitive option for Bayesian model fitting<br />
</li>
<li>nimble’s default random walk MCMC is not a great algorithm<br />
</li>
<li>No Monte Carlo algorithm is ever fully automatic<br />
</li>
<li>Entering the explanatory variables as constants rather than data overcomes the dynamic indexing problem.<br />
</li>
<li>The dynamic indexing and the dummy variables parameterisations gave similar results in similar run-times.<br />
</li>
<li>Playing with a beta version of a package is a process of trial and error<br />
</li>
<li>You need to be very careful with Object Orientated Programming (OOP) software, because it is in the nature of OOP that objects can change internally without you realising, which means that a second run of apparently identical code will not necessarily give the same results as a first run. Be careful.<br />
</li>
<li>The long run times that I experienced with some analyses seem to have been due to too short a warm-up leading to a poorly tuned algorithm. It was not a problem of setting up the automatic differentiation (A)D as I initially suspected.<br />
</li>
<li>Given the importance of tuning an algorithm to each specific problem, <code>nimble</code> needs to make it easy for the user to control the algorithm.</li>
</ul>
</div>
<div id="dynamic-indexing" class="section level2">
<h2>Dynamic Indexing</h2>
<p>In yesterday’s post, I started by writing my Poisson regression model in this form,</p>
<pre class="r"><code>library(nimble)

# --- Model code as used previously ------------------------------
nimbleCode( {
  for( i in 1:560 ) {
    log(mu[i]) &lt;- b0 + b1*year[i] + b2[age[i]] + b3*gender[i] + offset[i]
    deaths[i] ~ dpois(mu[i])
  }
  b0 ~ dnorm(0, 0.0001)
  b1 ~ dnorm(0, 0.0001)
  b2[1] &lt;- 0.0
  for(j in 2:14) {
    b2[j] ~ dnorm(0, 0.0001)
  }
  b3 ~ dnorm(0, 0.0001)
} )                       -&gt; modelCode</code></pre>
<p>Unfortunately, nimble could not cope with the expression <code>b2[age[i]]</code>, which caused a problem for the automatic differentiation (AD) needed by the HMC algorithm.</p>
<p>Almost as soon as I had pressed the button to publish the post, I realised that I had entered <code>age</code> as data, as you would in <code>OpenBUGS</code>, while <code>nimble</code> allows both data and constants.</p>
<p>I could have entered the explanatory variables as constants and the response variable as data as shown below.</p>
<pre class="r"><code># --- Explanatory data as constants ------------------------------
nimbleConst &lt;- list(offset = log(alcDF$pop),
                    year   = alcDF$year - 2001,
                    gender = as.numeric( alcDF$gender == &quot;male&quot;),
                    age    = as.numeric(alcDF$age) )

# --- response variable as data ----------------------------------
nimbleData &lt;- list( deaths = alcDF$deaths)

# --- initial values (as before) ---------------------------------
nimbleInits &lt;- 
  list( b1=0, b2=c( NA, rep(0,13)), b3=0)</code></pre>
<p>Now the model becomes</p>
<pre class="r"><code># --- create the model ---------------------------------
nimbleModel(
  code      = modelCode,
  data      = nimbleData,
  constants = nimbleConst,
  inits     = nimbleInits,
  buildDerivs = TRUE ) -&gt; model</code></pre>
<p>This compiles without problem, so we can continue as before.</p>
<pre class="r"><code>library(nimbleHMC)

# --- Compile the model ---------------------------------
modelCompiled &lt;- compileNimble(model)

# --- select the samplers ---------------------------------
modelHMC &lt;- buildHMC(model)

# --- compile the sampling algorithm ----------------------
hmcCompiled &lt;- compileNimble(modelHMC, project=model)

# --- run the compiled nimble code ----------------------
runMCMC(
  mcmc    = hmcCompiled,
  niter   = 1000,
  setSeed = 1832
)  %&gt;%
  saveRDS( file.path( home, &quot;data/dataStore/testHMC01.rds&quot;)) %&gt;%
  system.time()</code></pre>
<p>Running this code I found a huge increase in run-time, what has previously taken well under 10 seconds was now taking 75 seconds.</p>
<p>I thought at first that I might have been running some R code instead of the compiled code, I know that both methods are possible within nimble. Then I discovered that if I re-run the <code>runMCMC()</code> command without any recompilation, the run-time drops from 75 seconds to 9 seconds.</p>
<p>A possible explanation is that when first running the code, nimble needs to work out the details of the algorithm for calculating the derivatives, essentially through a series of applications of the chain rule. Once that algorithm is established, it is repeated and the calculation is relatively quick. This method is not forgotten, so when the chain is rerun, the run-time drops enormously. For this example, it looks as though it takes over a minute to establish the AD algorithm, which seems unrealistically long.</p>
<p>This raises the question of whether it would be quicker if I were to use yesterday’s reparameterisation in terms of dummy age variables.</p>
<p>I decided to run a few tests. I am afraid that those tests were rather quick and dirty in the sense that I ran each configuration only once. Run-times can vary depending on what other software is running in the background on the computer, so do not read too much into a run-time differences of few seconds either way. None the less, it does appear that the big determining factor is between the first run and any subsequent run. Parameterisation or movement of the explanatory variables between the data and the constants made almost no difference.</p>
</div>
<div id="the-trouble-with-oop" class="section level2">
<h2>The trouble with OOP</h2>
<p>Deep inside, <code>nimble</code> uses the R6 package to implement Object Orientated Programming (OOP). I am not particularly keen on OOP for one main reason; objects are mutable, that is to say they can change without you being aware of it. This has its uses, but it makes it impossible to trace exactly what has happened when you run code that you did not write.</p>
<p>In this case, nimble seems to remember the state of the chain, so that a second run picks up from where the previous run ended, which could be very useful. The danger here is that when running OOP code, it is easy to become confused whether you are actually running from scratch and this will have an impact on both timing and performance. The fact that yesterday my timing for 1000 iterations was so quick suggests that I had run the same code before without realising it. This means that the convergence of the chain might also be misleading. I need to go back and ensure that everything is run from scratch.</p>
</div>
<div id="lets-start-again" class="section level2">
<h2>Let’s start again</h2>
<p>Just so that we are clear, the code above with age entered as a constant and dynamic indexing takes about 75 seconds to run first time and about 9 seconds to re-run.</p>
<p>Here is a trace plot of the chain for the year coefficient, obtained from the the first run after I had started a fresh R session.</p>
<pre class="r"><code>library(MyPackage)

# --- read the result -----------------------------------------
readRDS(file.path( home, &quot;data/dataStore/testHMC01.rds&quot;)) %&gt;% 
  nimble_to_df() -&gt; simDF

# --- trace plot of b1 -------------------------------
trace_plot(simDF, b1, iter) +
  labs(title=&quot;HMC: Year coefficient (median and 80% interval)&quot;)</code></pre>
<p><img src="/post/bayes_nimble_hmc/methods_nimble_hmc2_files/figure-html/unnamed-chunk-6-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The performance is terrible, nothing like the performance that I got yesterday. Is this due to parameterisation or to inadvertently re-running yesterday’s chain.</p>
<p>I’ll re-run the same code. This takes about 7 seconds.</p>
<pre class="r"><code># --- run the compiled nimble code ----------------------
runMCMC(
  mcmc    = hmcCompiled,
  niter   = 1000,
  setSeed = 1832
)  %&gt;%
  saveRDS( file.path( home, &quot;data/dataStore/testHMC02.rds&quot;)) %&gt;%
  system.time()</code></pre>
<p>This is the new trace plot.</p>
<pre class="r"><code># --- read the result -----------------------------------------
readRDS(file.path( home, &quot;data/dataStore/testHMC02.rds&quot;)) %&gt;% 
  nimble_to_df() -&gt; simDF

# --- trace plot of b1 -------------------------------
trace_plot(simDF, b1, iter) +
  labs(title=&quot;HMC: Year coefficient (second run)&quot;)</code></pre>
<p><img src="/post/bayes_nimble_hmc/methods_nimble_hmc2_files/figure-html/unnamed-chunk-8-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Clearly, nimble has built the second run on what it learnt during the first run.</p>
<p>Here is the trace plot following a third run</p>
<pre class="r"><code># --- read the result -----------------------------------------
readRDS(file.path( home, &quot;data/dataStore/testHMC03.rds&quot;)) %&gt;% 
  nimble_to_df() -&gt; simDF

# --- trace plot of b1 -------------------------------
trace_plot(simDF, b1, iter) +
  labs(title=&quot;HMC: Year coefficient (third run)&quot;)</code></pre>
<p><img src="/post/bayes_nimble_hmc/methods_nimble_hmc2_files/figure-html/unnamed-chunk-9-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>We seem to be close to convergence for <code>b1</code>, but how about the other parameters. Here is a trace plot of the second age term obtained from the third run of the algorithm.</p>
<pre class="r"><code># --- trace plot of b2_2 -------------------------------
trace_plot(simDF, b2_2, iter) +
  labs(title=&quot;HMC: Age2 coefficient (third run)&quot;)</code></pre>
<p><img src="/post/bayes_nimble_hmc/methods_nimble_hmc2_files/figure-html/unnamed-chunk-10-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Even on the third run, this still resembles the results of a poor random walk MCMC sampler.</p>
<p>It is not too surprising that the age coefficients are less well estimated. <code>b2_2</code> is the contrast between age group 2 and the base age and can only be estimated from a fraction of the data, what is more, the coefficient will be correlated with the intercept, making convergence even slower.</p>
<p>Is the poor performance due to the parameterisation? I ran the version with dummy variables in place of dynamic indexes and repeated it three times as above. Here is the result for <code>b2_2</code></p>
<pre class="r"><code># --- read the result -----------------------------------------
readRDS(file.path( home, &quot;data/dataStore/testHMC04.rds&quot;)) %&gt;% 
  nimble_to_df() -&gt; simDF

trace_plot(simDF, b2_2, iter) +
  labs(title=&quot;HMC: Age2 coefficient (dummy variable parameterisation)&quot;)</code></pre>
<p><img src="/post/bayes_nimble_hmc/methods_nimble_hmc2_files/figure-html/unnamed-chunk-11-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Not much to choose between the parameterisations.</p>
</div>
<div id="improving-performance" class="section level2">
<h2>Improving performance</h2>
<p>In my limited experience, <code>stan</code> does well not just because it uses HMC, but also because the programmers have put a lot of effort into tuning the algorithm to the needs of each particular problem. This includes the way that they have used the warm-up period to select good control parameters and to make sure that the actual run starts close to the centre of the posterior.</p>
<p>My short runs of 1000 iterations trigger a warm-up of 500 iterations, taken together with my poor initial values this might well be too short.</p>
<p>Let’s try a longer run with better starting values. I’ll base my starting values on the equivalent generalised linear model.</p>
<pre class="r"><code># --- Poisson regression using glm() ------------------------------------------
library(broom)

glm( deaths ~ year + age + gender, 
     data   = alcDF %&gt;% mutate( year = year - 2001), 
     family = &quot;poisson&quot;, 
     offset = log(pop)) %&gt;%
  tidy()</code></pre>
<pre><code>## # A tibble: 16 × 5
##    term        estimate std.error statistic   p.value
##    &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
##  1 (Intercept)  -1.68    0.0586       -28.7 4.82e-181
##  2 year          0.0143  0.000465      30.7 4.63e-207
##  3 age25-29      1.50    0.0646        23.2 7.12e-119
##  4 age30-34      2.53    0.0606        41.7 0        
##  5 age35-39      3.25    0.0594        54.7 0        
##  6 age40-44      3.81    0.0590        64.5 0        
##  7 age45-49      4.18    0.0588        71.1 0        
##  8 age50-54      4.37    0.0587        74.4 0        
##  9 age55-59      4.44    0.0587        75.6 0        
## 10 age60-64      4.42    0.0588        75.2 0        
## 11 age65-69      4.26    0.0589        72.3 0        
## 12 age70-74      3.90    0.0593        65.7 0        
## 13 age75-79      3.53    0.0602        58.7 0        
## 14 age80-84      3.06    0.0624        49.0 0        
## 15 age85+        2.41    0.0671        35.9 1.60e-281
## 16 gendermale    0.778   0.00571      136.  0</code></pre>
<p>This suggests better starting values would be</p>
<pre class="r"><code>nimbleInits &lt;- list( b0 = -2, b1 = 0, 
                     b2 = c( NA, rep(3, 13)), b3 = 1)</code></pre>
<p>Running a 1000 iteration chain from these starting values produces</p>
<pre class="r"><code># --- read the result -----------------------------------------
readRDS(file.path( home, &quot;data/dataStore/testHMC05.rds&quot;)) %&gt;% 
  nimble_to_df() -&gt; simDF

trace_plot(simDF, b2_2, iter) +
  labs(title=&quot;HMC: Age2 coefficient (better starting values)&quot;)</code></pre>
<p><img src="/post/bayes_nimble_hmc/methods_nimble_hmc2_files/figure-html/unnamed-chunk-14-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Really no better. The algorithm might have found the centre of the posterior more quickly, but mixing is still poor.</p>
<p>How about a longer warm-up. I cleared everything and then ran from scratch with the better initial values and 3000 iterations. The warm-up jumped from 500 to 1000 iterations.</p>
<p>Here is the performance after a single run of 3000.</p>
<pre class="r"><code># --- read the result -----------------------------------------
readRDS(file.path( home, &quot;data/dataStore/testHMC06.rds&quot;)) %&gt;% 
  nimble_to_df() -&gt; simDF

trace_plot(simDF, b2_2, iter) +
  labs(title=&quot;HMC: Age2 coefficient (3000 iterations)&quot;)</code></pre>
<p><img src="/post/bayes_nimble_hmc/methods_nimble_hmc2_files/figure-html/unnamed-chunk-15-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Except for the very start of the chain, the performance looks better</p>
<pre class="r"><code>trace_plot(simDF %&gt;% filter(iter &gt; 500), b2_2, iter) +
  labs(title=&quot;HMC: Age2 coefficient (discarding the first 500 iterations)&quot;)</code></pre>
<p><img src="/post/bayes_nimble_hmc/methods_nimble_hmc2_files/figure-html/unnamed-chunk-16-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>There is a good algorithm in there somewhere, but it is not automatic.</p>
</div>
<div id="a-spanner-in-the-works" class="section level2">
<h2>A spanner in the works</h2>
<p>My first time run of length 3000 ran in <strong>20 seconds</strong>!!</p>
<p>Remember, it took 75 seconds to run 1000 iterations and I put this down to initialising the AD. Perhaps that was not correct. Could it be that the slow time for the shorter run was due to poor tuning of the algorithm in the correspondingly shorter warm-up.</p>
<p>I tried setting the <code>nburnin</code> parameter of <code>runMCMC()</code> to 500. This had the effect to discarding the first 500 iterations, but the warm-up was still of length 1000.</p>
<pre class="r"><code># --- read the result -----------------------------------------
readRDS(file.path( home, &quot;data/dataStore/testHMC07.rds&quot;)) %&gt;% 
  nimble_to_df() -&gt; simDF

trace_plot(simDF, b2_2, iter) +
  labs(title=&quot;HMC: Age2 coefficient (with a burn-in of 500)&quot;)</code></pre>
<p><img src="/post/bayes_nimble_hmc/methods_nimble_hmc2_files/figure-html/unnamed-chunk-17-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>As far as I can tell, there is no way of requesting a longer warm-up. The length seems to be set automatically.</p>
</div>
<div id="how-about-mcmc" class="section level2">
<h2>How about MCMC</h2>
<p>One final comparison, I ran the default MCMC with random walk samplers for all of the parameters. I decided on a run of 10,000 with the first 5,000 discarded.</p>
<p>Mixing is still quite poor although the algorithm has found the right region.</p>
<pre class="r"><code># --- read the result -----------------------------------------
readRDS(file.path( home, &quot;data/dataStore/testMCMC08.rds&quot;)) %&gt;% 
  nimble_to_df() -&gt; simDF

trace_plot(simDF, b2_2, iter) +
  labs(title=&quot;MCMC: Age2 coefficient (median and 80% interval)&quot;)</code></pre>
<p><img src="/post/bayes_nimble_hmc/methods_nimble_hmc2_files/figure-html/unnamed-chunk-18-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>On the plus side, this run took under 4 seconds. This means that in the 20 seconds that HMC took to give me 2500 usable simulations, I could have run an MCMC chain of length 50,000 and discarded the first 5,000 and thinned by 18 to leave me with 2,500 usable simulations. I tried it and it did take about 20 seconds.</p>
<pre class="r"><code># --- read the result -----------------------------------------
readRDS(file.path( home, &quot;data/dataStore/testMCMC09.rds&quot;)) %&gt;% 
  nimble_to_df() -&gt; simDF

trace_plot(simDF, b2_2, iter) +
  labs(title=&quot;MCMC: Age2 coefficient (50,000 thinned by 18)&quot;)</code></pre>
<p><img src="/post/bayes_nimble_hmc/methods_nimble_hmc2_files/figure-html/unnamed-chunk-19-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Nice idea but the mixing is still terrible, far worse than the corresponding HMC run.</p>
<p>I know that I could centre the x’s, use a block sampler for the correlated parameters and tune the standard deviation of the random walk, but the fundamental problem remains; RW Metropolis-Hastings is not a great algorithm.</p>
</div>
<div id="conclusions" class="section level2">
<h2>Conclusions</h2>
<p>So back to where I started with my conclusions</p>
<ul>
<li>I remain optimistic that HMC will make nimble a more competitive option for Bayesian model fitting<br />
</li>
<li>nimble’s default random walk MCMC is not a great algorithm<br />
</li>
<li>No Monte Carlo algorithm is ever fully automatic<br />
</li>
<li>Entering the explanatory variables as constants rather than data overcomes the dynamic indexing problem.<br />
</li>
<li>The dynamic indexing and the dummy variables parameterisations gave similar results in similar run-times.<br />
</li>
<li>Playing with a beta version of a package is a process of trial and error<br />
</li>
<li>You need to be very careful with Object Orientated Programming (OOP) software, because it is in the nature of OOP that objects can change internally without you realising, which means that a second run of apparently identical code will not necessarily give the same results as a first run. Be careful.<br />
</li>
<li>The long run times that I experienced with some analyses seem to have been due to too short a warm-up leading to a poorly tuned algorithm. It was not a problem of setting up the automatic differentiation (A)D as I initially suspected.<br />
</li>
<li>Given the importance of tuning an algorithm to each specific problem, <code>nimble</code> needs to make it easy for the user to control the algorithm.</li>
</ul>
</div>
