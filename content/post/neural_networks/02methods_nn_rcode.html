---
title: "Neural Networks: R Code for Gradient Descent"
author: "John Thompson"
date: "2023-08-25"
layout: post
categories:
- neural networks
- gradient descent
- back-propagation
output:
    html_document:
    keep_md: true
editor_options:
  chunk_output_type: console
---

<script src="/rmarkdown-libs/htmlwidgets/htmlwidgets.js"></script>
<script src="/rmarkdown-libs/viz/viz.js"></script>
<link href="/rmarkdown-libs/DiagrammeR-styles/styles.css" rel="stylesheet" />
<script src="/rmarkdown-libs/grViz-binding/grViz.js"></script>


<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>In this post, I present my R code for fitting a neural network (NN) using gradient descent. I know before I start that R is too slow for the code to be of practical use, but writing it helps me understand the algorithm and in my next post I’ll speed up the execution by rewriting the key functions in C++ using the <code>Rcpp</code> package.</p>
<div id="creating-predictions" class="section level2">
<h2>Creating predictions</h2>
<p>First a recap; in my introductory post on NNs, I used the example of a NN with 8 nodes arranged in 4 layers and explained that I like to number the nodes in order, in this case 1 to 8, and to give each node a value denoted <span class="math inline">\(v_1\)</span> to <span class="math inline">\(v_8\)</span>.</p>
<div class="grViz html-widget html-fill-item-overflow-hidden html-fill-item" id="htmlwidget-1" style="width:672px;height:480px;"></div>
<script type="application/json" data-for="htmlwidget-1">{"x":{"diagram":"\ndigraph dot {\n\ngraph [layout = dot,\n       rankdir = LR]\n\nnode [shape = circle,\n      style = filled,\n      color = black,\n      label = \"\"]\n\nnode [fillcolor = mistyrose]\na [label = \"v1\"] \nb [label = \"v2\"] \n\nnode [fillcolor = LightCyan]\nc  [label = \"v3\"]\nd  [label = \"v4\"] \ne  [label = \"v5\"]\nf  [label = \"v6\"] \ng  [label = \"v7\"] \n\nnode [fillcolor = orange]\nh  [label = \"v8\"]\n\nedge [color = grey]\na -> {c d e}\nb -> {c d e}\nc -> {f g}\nd -> {f g}\ne -> {f g}\nf -> h\ng -> h\n}","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
<p>The first two nodes make up the input layer and are set equal to the values of the predictors, x1 and x2,
<span class="math display">\[
v_1 = x_{1} \ \ \ \ \text{and} \ \ \ v_2 = x_{2}
\]</span></p>
<p>The calculation for each subsequent node can be split into two stages. Using <span class="math inline">\(v_3\)</span> as an example, the first stage calculates <span class="math inline">\(z_3\)</span> as a simple linear combination of the inputs,<br />
<span class="math display">\[
z_3 = \beta_3 + \beta_{13} v_1 + \beta_{23} v_2
\]</span>
The second stage converts <span class="math inline">\(z_3\)</span> into <span class="math inline">\(v_3\)</span> using the activation function, f().<br />
<span class="math display">\[
v_3 = f(z_3)
\]</span></p>
<p>In my R code, I place the values of <code>z</code>, <code>v</code> and the biases in vectors of length 8, even though the first two elements of <code>v</code> have known values and the corresponding elements of <code>z</code> and the biases will never be used.</p>
<p>Because the calculations are performed, layer by layer, it is convenient to have a pointer that tells where within these vectors each layer begins and ends. In this case, the pointer is (1, 3, 6, 8), meaning that layer 2 starts with node 3 and finishes with node 5 (6-1).</p>
<p>There are 14 weights represented by arrows in the diagram. These weights are placed in a vector in the order (13, 14, 15, 23, 24, 25, 36, 37, 46, 47, 56, 57, 68, 78), where 37 means the weight connecting node 3 to node 7. A second pointer that identifies the layer of origin of each arrow, In this case, it is (1, 7, 13)</p>
<p>Eventually, the R code will need to create these design variables from a description of the network’s architecture, but for the moment I will manually create values specific to this example and also generate random weights and biases.</p>
<pre class="r"><code># first node of each layer
nPtr &lt;- c(1, 3, 6, 8, 9)
# starting and finishing node for each weight
from &lt;- c(1, 1, 1, 2, 2, 2, 3, 3, 4, 4, 5, 5, 6, 7)
to   &lt;- c(3, 4, 5, 3, 4, 5, 6, 7, 6, 7, 6, 7, 8, 8)
# first weight of each layer
wPtr &lt;- c(1, 7, 13, 15)
# Space for v and z
v &lt;- rep(0, 8)
z &lt;- rep(0, 8)
# Random values for the bias and weight
set.seed(5561)
bias &lt;- runif(8, -1, 1)
weight &lt;- runif(14, -1, 1)</code></pre>
<p>For coding convenience my pointers include a extra value that is one more than the length of the corresponding vector.</p>
<p>I apply a sigmoid activation function to the hidden nodes, this will convert <code>z</code> to an activated value, v, that lies between 0 and 1. The plot below shows the shape of this activation function.</p>
<pre class="r"><code>z &lt;- seq(-4, 4, 0.1)
v &lt;- 1 /( 1 + exp(-z))
plot(z, v, type = &quot;l&quot;, main = &quot;Sigmoid Activation Function&quot;)</code></pre>
<p><img src="/post/neural_networks/02methods_nn_rcode_files/figure-html/unnamed-chunk-4-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The sigmoid is a popular choice, but many others have been suggested.</p>
<p>Since, in my regression example, y is unbounded, I will not apply any activation to the final prediction but instead set <span class="math inline">\(v_8 = z_8\)</span>.</p>
<p>Suppose that the first observation in the training data is (x1 = 3.1, x2 = 2.7, y = 0.3). The following code calculates the value <code>v</code> of each node. <code>v</code> includes the predicted value of y, which is found in the output layer.</p>
<p>The weights and biases have been set randomly, so we should not expect the prediction from this NN to be accurate.</p>
<pre class="r"><code># set the values of the input layer
v    &lt;- rep(0, 8)
v[1] &lt;- 3.1
v[2] &lt;- 2.7
# set z equal to the bias
z &lt;- bias
# for layers 2 to 4
for(i in 2:4) {
  # for each node in this layer
  for(j in nPtr[i]:(nPtr[i+1]-1)) {
    # weights that originate in the previous layer
    for(h in wPtr[i-1]:(wPtr[i]-1)) {
      # does the weight go to node j?
      if( to[h] == j ) {
        # add (weight * value) to z
        z[j] &lt;- z[j] + weight[h] * v[from[h]]
      }
    }
    # apply the sigmoid activation function to hidden nodes
    if( i != 4 ) {
      v[j] &lt;- 1.0 / (1.0 + exp(-z[j]))
    } else {
      v[j] &lt;- z[j]
    }
  }
}
print(v)</code></pre>
<pre><code>## [1]  3.10000000  2.70000000  0.07676566  0.56151852  0.98352402  0.74555848
## [7]  0.68832585 -0.59856899</code></pre>
<p>As expected, these random weights produce a poor prediction, <span class="math inline">\(v_8\)</span>, of -0.5986, when y is actually 0.3.</p>
<p>For illustration, I use the usual mean square loss
<span class="math display">\[
L(y, \mu) = \frac{1}{n} \sum_{i = 1}^n (y_i - \mu_i)^2
\]</span></p>
<p>So for the single training observation the loss is 0.807.</p>
<pre class="r"><code>y &lt;- 0.3
loss &lt;- (y - v[8])^2
print(loss)</code></pre>
<pre><code>## [1] 0.8074262</code></pre>
</div>
<div id="gradient-descent" class="section level2">
<h2>Gradient Descent</h2>
<p>Random parameters give poor estimates and a high loss, but they act as starting values for an algorithm that progressively reduces the loss.</p>
<p>In order to select appropriate adjustments to the weights and biases, I calculate the derivative of the loss with respect to each of the 20 parameters (weights + biases). Then I use these derivatives to guide a small adjustment in a downhill direction. There are no guarantees that changing all of the parameters at once in this way will reduce the loss, but it is a reasonable strategy that works most of the time, especially when the step sizes of the adjustments are kept small.</p>
<p>Computing the 20 derivatives in a way that is generalisable is easier that it might at first seem, provided that we work backwards through the network. The algorithm that calculates the derivatives in this way is known as <code>back-propagation</code>.</p>
<p>In terms of the <code>v</code>’s the loss function is
<span class="math display">\[
L(y, \mu) = \frac{1}{n} \sum_{i = 1}^n (y_i - v_{8i})^2
\]</span>
For simplicity, I’ll drop the i suffix that refers to the ith observation in the training data. In practice, the algorithm cycles through each training observation in turn and sums the results, so all we really need is R code that works for a single training observation. The simplified loss is,
<span class="math display">\[
L(y, \mu) = (y - v_8)^2
\]</span>
and the derivative of the loss with respect to <span class="math inline">\(v_8\)</span> is
<span class="math display">\[
\frac{\partial L}{\partial v_8} = -2 (y - v_8)
\]</span></p>
<p>Since no activation function is applied at the final stage, <span class="math inline">\(z_8 = v_8\)</span> and
<span class="math display">\[
\frac{\partial L}{\partial z_8} = \frac{\partial L}{\partial v_8}
\]</span></p>
<p>The diagram of the network shows that
<span class="math display">\[
z_8 = \beta_8 + \beta_{68} v_6 + \beta_{78} v_7
\]</span>
where <span class="math inline">\(v_6\)</span> and <span class="math inline">\(v_7\)</span> have already been calculated as part of the forward step that found the prediction, <span class="math inline">\(v_8\)</span>.</p>
<p>The derivative of the loss with respect to the bias, <span class="math inline">\(\beta_8\)</span>, is
<span class="math display">\[
\frac{\partial L}{\partial \beta_8} = \frac{\partial L}{\partial z_8} \frac{\partial z_8}{\partial \beta_8} = -2 (y - v_8) \ \ \text{x} \ \ 1
\]</span>
and the derivative of the loss with respect to the weight, <span class="math inline">\(\beta_{68}\)</span>, is
<span class="math display">\[
\frac{\partial L}{\partial \beta_{68}} = \frac{\partial L}{\partial z_8} \frac{\partial z_8}{\partial \beta_{68}} = -2 (y - v_8) \ \ \text{x} \ \ v_6
\]</span>
A similar formula gives the derivative with respect to <span class="math inline">\(\beta_{78}\)</span>,
<span class="math display">\[
\frac{\partial L}{\partial \beta_{78}} = \frac{\partial L}{\partial z_8} \frac{\partial z_8}{\partial \beta_{78}} = -2 (y - v_8) \ \ \text{x} \ \ v_7
\]</span></p>
<p>We can also calculate the derivative with respect to <span class="math inline">\(v_6\)</span>. Although this is not of direct interest, it is needed when the next layer of derivatives is calculated.
<span class="math display">\[
\frac{\partial L}{\partial v_6} = \frac{\partial L}{\partial z_8} \frac{\partial z_8}{\partial v_6} = -2 (y - v_8) \ \ \text{x} \ \ \beta_{68}
\]</span>
where <span class="math inline">\(\beta_{68}\)</span> is the current value of that weight.</p>
<p>Moving backwards through the NN from layer 4 to layer 3,
<span class="math display">\[
z_6 = \beta_6 + \beta_{36} v_3 + \beta_{46} v_4 + \beta_{56} v_5
\]</span>
A sigmoid activation function is used in this layer, so I need to include the derivative of <span class="math inline">\(v_6\)</span> with respect to <span class="math inline">\(z_6\)</span>. Since <span class="math inline">\(v_6 = f(z_6)\)</span>, this is just the derivative of the activation function. In the case of the sigmoid, this derivative is <span class="math inline">\(v_6(1-v_6)\)</span></p>
<p>Putting this all together, the derivative with respect to the bias, <span class="math inline">\(\beta_6\)</span>, is,
<span class="math display">\[
\frac{\partial L}{\partial \beta_6} = \frac{\partial L}{\partial v_6} \frac{\partial v_6}{\partial z_6} \frac{\partial z_6}{\partial \beta_6}
\]</span>
I calculated the partial with respect to <span class="math inline">\(v_6\)</span> in the previous layer and the partial of <span class="math inline">\(z_6\)</span> with respect to <span class="math inline">\(\beta_6\)</span> is one.</p>
<p>This pattern continues as we move backwards through the network. It is very repetitive, so, with care, it can be coded.</p>
<p>In my code, I create three vectors of length 8 to hold the derivatives of the loss with respect to (wrt) the <code>bias</code> and wrt <code>v</code> and the derivative of the activation function. A vector of length 14 holds the derivatives of the loss wrt the weights.</p>
<pre class="r"><code># vectors for the derivatives
dbias &lt;- dv &lt;- df &lt;- rep(0, 8)
dweight &lt;- rep(0, 14)
# the training response
y &lt;- 0.3
# derivatives of the activation functions
for(j in 3:7) {
  df[j] &lt;- v[j]*(1-v[j])
}
df[8] &lt;- 1
# final layer dloss/da
dv[8] &lt;- -2 * (y - v[8])
# move backwards through the layers
for(i in 4:2) {
  # for each node in that layer
  for( j in nPtr[i]:(nPtr[i+1]-1) ) {
     # dloss/dbias
     dbias[j] &lt;- dv[j] * df[j] * 1
     # dloss/dweight
     # for weights that end at node j
     for( h in wPtr[i-1]:(wPtr[i]-1) ) {
        if( to[h] == j ) {
           dweight[h] &lt;- dv[j] * df[j] * v[from[h]]
        }
     }
  }
  # dloss/dv - may involve multiple routes
  for( j in nPtr[i-1]:(nPtr[i]-1) ) {
     dv[j] &lt;- 0
     for( k in nPtr[i]:(nPtr[i+1]-1) ) {
        for(h in wPtr[i-1]:(wPtr[i]-1) ) {
           if( from[h] == j &amp; to[h] == k) wjk &lt;- weight[h]
        }
        dv[j] &lt;- dv[j] + dv[k] * df[k] * wjk
     }
  }
}
print(dbias)</code></pre>
<pre><code>## [1]  0.0000000000  0.0000000000 -0.0020395917 -0.0088170206  0.0001177478
## [6] -0.0347579283 -0.1432396942 -1.7971379872</code></pre>
<pre class="r"><code>print(dweight)</code></pre>
<pre><code>##  [1] -0.0063227343 -0.0273327637  0.0003650183 -0.0055068976 -0.0238059555
##  [6]  0.0003179192 -0.0026682155 -0.0109958903 -0.0195172206 -0.0804317416
## [11] -0.0341852575 -0.1408796803 -1.3398714709 -1.2370165354</code></pre>
<p>These derivatives tell us the direction to move in. To reduce the loss we must move against the gradient. I’ll make my step length, <code>eta</code>, equal to 0.1. In machine learning <code>eta</code> is usually called the learning rate.</p>
<pre class="r"><code># step length
eta &lt;- 0.1
# steps down hill
weight &lt;- weight - eta * dweight
bias   &lt;- bias   - eta * dbias</code></pre>
<p>With these adjusted parameters we can recalculate the prediction and the loss</p>
<pre class="r"><code># repeat the forward pass to create a new prediction
z &lt;- bias
for(i in 2:4) {
  for(j in nPtr[i]:(nPtr[i+1]-1)) {
    for(h in wPtr[i-1]:(wPtr[i]-1)) {
      if( to[h] == j ) {
        z[j] &lt;- z[j] + weight[h] * v[from[h]]
      }
    }
    if( i != 4 ) {
      v[j] &lt;- 1.0 / (1.0 + exp(-z[j]))
    } else {
      v[j] &lt;- z[j]
    }
  }
}
print(v[8])</code></pre>
<pre><code>## [1] -0.229929</code></pre>
<pre class="r"><code>loss &lt;- (y - v[8])^2
print(loss)</code></pre>
<pre><code>## [1] 0.2808248</code></pre>
<p>The loss has been reduced from 0.807 to 0.281. Now we just repeat and repeat.</p>
</div>
<div id="nn-fitting-functions" class="section level2">
<h2>NN Fitting Functions</h2>
<p>In the R code below, I have generalized this fitting process so that it works with any neural network architecture. The user must specify the number of nodes in each layer, the loss function, the activation function and their derivatives.</p>
<p>The function <code>prepare_nn()</code> will generate random starting values together with pointers to the start of each layer and it also creates the <code>to</code> and <code>from</code> vectors that link the weights to the nodes.</p>
<pre class="r"><code># Takes the architecture arch and creates pointers and random starting values
# arch = c(2, 3, 2, 1) means 4 layers with 2 predictors and 1 response 
prepare_nn &lt;- function(arch) {
  nLayers &lt;- length(arch)
  nNodes  &lt;- sum(arch)
  nPtr &lt;- rep(0, nLayers+1)
  # pointer to the first node in a layer
  h &lt;- 1
  for( j in 1:nLayers) {
    nPtr[j] &lt;- h
    h &lt;- h + arch[j]
  }
  nPtr[nLayers+1] &lt;- nNodes+1
  # number of weights
  nWt &lt;- 0
  for( j in 2:nLayers) {
    nWt &lt;- nWt + arch[j-1] * arch[j]
  }
  # node origin and destination of each weight
  from &lt;- to &lt;- rep(0, nWt)
  q1 &lt;- q2 &lt;- h &lt;- 0
  for( i in 2:nLayers) {
    q1 &lt;- q1 + arch[i-1]
    for(f in 1:arch[i-1]) {
      for(t in 1:arch[i]) {
        h &lt;- h + 1
        from[h] &lt;- f + q2
        to[h]   &lt;- t + q1
      }
    }
    q2 &lt;- q1
  }
  # pointer to the first weight in each layer
  wPtr&lt;- rep(0, nLayers)
  for( h in 1:nWt) {
    for( i in 1:(nLayers-1)) {
      if( from[h] == nPtr[i] &amp; wPtr[i] == 0 ) {
        wPtr[i] &lt;- h  
      }
    }
  }
  wPtr[nLayers] &lt;- nWt + 1
  # random starting values
  bias &lt;- runif(nNodes, -1, 1)
  weight &lt;- runif(nWt, -1, 1)
  # return the design
  return( list(bias = bias, weight = weight, 
               from = from, to = to, 
               nPtr = nPtr, wPtr = wPtr))
}</code></pre>
<p>To test the code, I run this function for the demonstration network and get the same design as I entered manually earlier in this post.</p>
<pre class="r"><code>set.seed(5561)
design &lt;- prepare_nn( c(2, 3, 2, 1))
str(design)</code></pre>
<pre><code>## List of 6
##  $ bias  : num [1:8] -0.7712 0.2912 -0.3997 0.4239 -0.0587 ...
##  $ weight: num [1:14] -0.423 0.688 0.502 -0.288 -0.855 ...
##  $ from  : num [1:14] 1 1 1 2 2 2 3 3 4 4 ...
##  $ to    : num [1:14] 3 4 5 3 4 5 6 7 6 7 ...
##  $ nPtr  : num [1:5] 1 3 6 8 9
##  $ wPtr  : num [1:4] 1 7 13 15</code></pre>
<p>The forward pass through the network that calculates the value for each node is coded in the function <code>forward_nn()</code>. The user needs to provide the activation function for the nodes in the hidden layers and a separate activation function for the nodes in the output layer.</p>
<pre class="r"><code># The activation functions
actHidden &lt;- function(z) {
  return(1.0 / (1.0 + exp(-z)))
}
actOutput &lt;- function(z) {
  return(z)
}</code></pre>
<p>The values of the vector <code>v</code> can now be calculated.</p>
<pre class="r"><code># Move forward through the network calculating each node&#39;s value, v
# inputs (predictors) must be entered into v before calling this function
forward_nn &lt;- function(v, design) {
  nLayers &lt;- length(design$nPtr) - 1
  # set z equal to the bias
  z &lt;- design$bias
  # for layers 2 onwards
  for(i in 2:nLayers) {
    # for each node in this layer
    for(j in design$nPtr[i]:(design$nPtr[i+1]-1)) {
      # weights that originate from layer i-1
      for(h in design$wPtr[i-1]:(design$wPtr[i]-1)) {
        # does the weight go to node j?
        if( design$to[h] == j ) {
          # add (weight * value) to z
          z[j] &lt;- z[j] + design$weight[h] * v[design$from[h]]
        }
      }
      # apply the sigmoid activation function to hidden nodes
      if( i != nLayers ) {
        v[j] &lt;- actHidden(z[j])
      } else {
        v[j] &lt;- actOutput(z[j])
      }
    }
  }
  return(v)
}</code></pre>
<p>Once again, I use the demonstration NN as a test</p>
<pre class="r"><code>v &lt;- rep(0, length(design$bias))
v[1] &lt;- 3.1
v[2] &lt;- 2.7
v &lt;- forward_nn(v, design)
print(v)</code></pre>
<pre><code>## [1]  3.10000000  2.70000000  0.07676566  0.56151852  0.98352402  0.74555848
## [7]  0.68832585 -0.59856899</code></pre>
<p>Reassuringly, I get the same prediction as before, <span class="math inline">\(v_8\)</span> = -0.5986.</p>
<p>Next, I need a function for back-propagation. At this stage, the user must specify their loss and the derivatives of that loss and the derivatives of the activation functions.</p>
<pre class="r"><code># loss function
loss &lt;- function(y, yhat) {
  return( (y-yhat)^2 )
}
# derivative of loss wrt prediction
dLoss &lt;- function(y, yhat) {
  return( -2*(y-yhat))
}
# derivative of hidden layer activation
dActHidden &lt;- function(v) {
  return(v*(1-v))
}
# derivative of output layer activation
dActOutput &lt;- function(v) {
  return(1)
}</code></pre>
<p>With these functions defined, back-propagation is performed by the function <code>backprop_nn()</code>.</p>
<pre class="r"><code># back-propagate the derivatives
backprop_nn &lt;- function(y, v, design) {
  nLayers &lt;- length(design$nPtr) - 1
  nNodes  &lt;- length(v)
  nWts    &lt;- length(design$weight)    
  dbias   &lt;- dv &lt;- df &lt;- rep(0, nNodes)
  dweight &lt;- rep(0, nWts)
  nX      &lt;- design$nPtr[2] - design$nPtr[1]
  nY      &lt;- length(y)
  yhat    &lt;- rep(0, nY)
  # derivatives of the activation functions
  for(j in nPtr[2]:(nNodes-nY) ) {
    df[j] &lt;- dActHidden(v[j])
  }
  for(j in (nNodes-nY+1):nNodes ) {
    df[j] &lt;- dActOutput(v[j])
    yhat[j-nNodes+nY] &lt;- v[j]
  }
  # final layer dloss/dv
  dv[nNodes] &lt;- dLoss(y, yhat)
  # move backwards through the layers
  for(i in nLayers:2) {
    # for each node in that layer
    for( j in design$nPtr[i]:(design$nPtr[i+1]-1) ) {
      # dloss/dbias
       dbias[j] &lt;- dv[j] * df[j]
       # dloss/dweight
       # for weight that ends at node j
       for( h in design$wPtr[i-1]:(design$wPtr[i]-1) ) {
         if( design$to[h] == j ) {
           dweight[h] &lt;- dv[j] * df[j] * v[design$from[h]]
         }
       }
    }
    # dloss/dv - may involve multiple routes
    for( j in design$nPtr[i-1]:(design$nPtr[i]-1) ) {
      dv[j] &lt;- 0
      for( k in design$nPtr[i]:(design$nPtr[i+1]-1) ) {
        for(h in design$wPtr[i-1]:(design$wPtr[i]-1) ) {
           if( design$from[h] == j &amp; design$to[h] == k) 
             wjk &lt;- design$weight[h]
        }
        dv[j] &lt;- dv[j] + dv[k] * df[k] * wjk
      }
    }
  }
  return(list(dbias = dbias, dweight = dweight))
}</code></pre>
<p>For the demonstration example the derivatives of the parameters are,</p>
<pre class="r"><code>deriv &lt;- backprop_nn(0.3, v, design)
str(deriv)</code></pre>
<pre><code>## List of 2
##  $ dbias  : num [1:8] 0 0 -0.00204 -0.008817 0.000118 ...
##  $ dweight: num [1:14] -0.006323 -0.027333 0.000365 -0.005507 -0.023806 ...</code></pre>
<p>Using these derivatives to adjust the parameters improves the loss exactly as it did when I used my problem specific code.</p>
<pre class="r"><code># old loss
ls &lt;- loss(0.3, v[8])
print(ls)</code></pre>
<pre><code>## [1] 0.8074262</code></pre>
<pre class="r"><code># adjust the parameters
eta &lt;- 0.1
design$bias   &lt;- design$bias   - eta * deriv$dbias
design$weight &lt;- design$weight - eta * deriv$dweight
# new prediction
v &lt;- forward_nn(v, design)
print(v)</code></pre>
<pre><code>## [1]  3.10000000  2.70000000  0.07702481  0.56540056  0.98352061  0.74663794
## [7]  0.69565422 -0.22992901</code></pre>
<pre class="r"><code># new loss
ls &lt;- loss(0.3, v[8])
print(ls)</code></pre>
<pre><code>## [1] 0.2808248</code></pre>
<p>My next function, <code>fit_nn()</code>, takes a set of training data saved in matrices X and Y and iterates the forward pass and back-propagation.</p>
<pre class="r"><code># fit a NN to training data (X, Y)
fit_nn &lt;- function(X, Y, design, eta = 0.1, nIter = 1000, trace=TRUE ) {
  # size of the training data
  nr &lt;- nrow(X)
  nX &lt;- ncol(X)
  nY &lt;- ncol(Y)
  # problem size and working variables
  nNodes  &lt;- length(design$bias)
  nWts    &lt;- length(design$weight)    
  v &lt;- rep(0, nNodes)
  yhat &lt;- rep(0, nY)
  lossHistory &lt;- rep(0, nIter)
  # iterate nIter times
  for( iter in 1:nIter) {
    # set derivatives &amp; loss to zero
    dw &lt;- rep(0, nWts)
    db &lt;- rep(0, nNodes)
    tloss &lt;- 0
    # iterate over the rows of the training data
    for( d in 1:nr ) {
       # set the predictors into a
       for(i in 1:nX) {
         v[i] &lt;- X[d, i]
       }
       # forward pass
       v &lt;- forward_nn(v, design)
       # extract the predictions
       for(i in 1:nY) {
         yhat[i] &lt;- v[nNodes-nY+i]
       }
       # calculate the loss 
       tloss &lt;- tloss + loss(as.numeric(Y[d, ]), yhat)
       # backpropagate
       deriv &lt;- backprop_nn(as.numeric(Y[d, ]), v, design)
       # sum the derivatives
       dw &lt;- dw + deriv$dweight
       db &lt;- db + deriv$dbias
    }
    # save loss and update the parameters
    lossHistory[iter] &lt;- tloss / nr
    design$weight &lt;- design$weight - eta * dw / nr
    design$bias   &lt;- design$bias   - eta * db / nr
    # print loss
    if( trace &amp; iter %% 100 == 0) {
      cat(iter, &quot; &quot;, tloss/nr, &quot;\n&quot;)
    }
  }
  # return the results
  return(list(bias = design$bias, weight = design$weight, 
              lossHistory = lossHistory,
              dbias = db/nr, dweight = dw/nr))
}</code></pre>
<p>Finally, there is a function to calculate the predictions for a test set of data X.</p>
<pre class="r"><code>predict_nn &lt;- function(X, design) {
  # size of the training data
  nr &lt;- nrow(X)
  nX &lt;- ncol(X)
  nLayers &lt;- length(design$nPtr) - 1;
  nNodes &lt;- length(design$bias);
  nY &lt;- design$nPtr[nLayers+1] - design$nPtr[nLayers];
  Y &lt;- matrix(0, nrow=nr, ncol=nY)
  v &lt;- rep(0, nNodes)
  for( d in 1:nr ) {
    # set the predictors into a
    for(i in 1:nX) {
      v[i] &lt;- X[d, i]
    }
    # forward pass
    v &lt;- forward_nn(v, design)
    # extract the predictions
    for(i in 1:nY) {
      Y[d, i] &lt;- v[nNodes-nY+i]
    }
  }
  return(Y)
}</code></pre>
<p>Let’s test this code with some simulated data. For ease of display, I have chosen a problem with a single predictor and a single output. The training set has 15 observations generated from a quadratic with a small amount of random error.</p>
<pre class="r"><code># Simulate some training data
set.seed(7109)
X &lt;- matrix(runif(15, 0, 1), nrow = 15, ncol = 1)
Y &lt;- matrix(3*(X[,1]-0.5)^2 + rnorm(15, 0, 0.2), nrow = 15, ncol = 1)
plot(X[, 1], Y[, 1], pch = 16, main = &quot;The training data&quot;)</code></pre>
<p><img src="/post/neural_networks/02methods_nn_rcode_files/figure-html/unnamed-chunk-21-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>I’ll model these data with a NN that has a single hidden layer containing 4 nodes.</p>
<pre class="r"><code># create the network design
set.seed(8923)
design &lt;- prepare_nn(c(1, 4, 1))
str(design)</code></pre>
<pre><code>## List of 6
##  $ bias  : num [1:6] 0.47234 0.00661 -0.42113 0.00113 -0.10775 ...
##  $ weight: num [1:8] 0.1419 -0.675 -0.5684 0.71 0.0493 ...
##  $ from  : num [1:8] 1 1 1 1 2 3 4 5
##  $ to    : num [1:8] 2 3 4 5 6 6 6 6
##  $ nPtr  : num [1:4] 1 2 6 7
##  $ wPtr  : num [1:3] 1 5 9</code></pre>
<p>When I run 1000 iterations, the loss is reduced a little. There is a big improvement in the loss in the first few iterations, but little change after that.</p>
<pre class="r"><code>fit &lt;- fit_nn(X, Y, design, eta=0.1, nIter = 1000)</code></pre>
<pre><code>## 100   0.1153072 
## 200   0.1144552 
## 300   0.1138409 
## 400   0.1133947 
## 500   0.1130678 
## 600   0.1128253 
## 700   0.1126414 
## 800   0.1124976 
## 900   0.1123801 
## 1000   0.1122794</code></pre>
<pre class="r"><code>plot(fit$lossHistory, type = &quot;l&quot;, ylim = c(0, 0.2))</code></pre>
<p><img src="/post/neural_networks/02methods_nn_rcode_files/figure-html/unnamed-chunk-23-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>We can visualise the model suggested by this NN by making predictions at a sequence of values between 0 and 1.</p>
<pre class="r"><code>design$bias   &lt;- fit$bias
design$weight &lt;- fit$weight
# selected values of x
xt &lt;- matrix(seq(0, 1, 0.02), ncol=1)
# make prediction for each xt
yt &lt;- predict_nn(xt, design)
# plot the predictions
plot(X[, 1], Y[, 1], pch = 16, main = &quot;NN model after 1000 iterations&quot;)
lines(xt[, 1], yt[, 1])</code></pre>
<p><img src="/post/neural_networks/02methods_nn_rcode_files/figure-html/unnamed-chunk-24-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The network has fitted a straightish line through the points. Hopefully, this will be improved upon when I run more iterations.</p>
<pre class="r"><code>start_time &lt;- Sys.time()
fit &lt;- fit_nn(X, Y, design, eta=0.1, nIter = 10000, trace=FALSE)
end_time &lt;- Sys.time()
print(end_time - start_time)</code></pre>
<pre><code>## Time difference of 3.928143 secs</code></pre>
<pre class="r"><code>plot(fit$lossHistory, type = &quot;l&quot;, ylim = c(0, 0.2))</code></pre>
<p><img src="/post/neural_networks/02methods_nn_rcode_files/figure-html/unnamed-chunk-25-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>It only takes a few seconds for the algorithm to locate a better solution.</p>
<p>Plotting the predictions from this latest model shows that the NN has discovered a better approximation to the shape of the response curve.</p>
<pre class="r"><code>design$bias   &lt;- fit$bias
design$weight &lt;- fit$weight
# make prediction for each xt
yt &lt;- predict_nn(xt, design)

# plot the predictions
plot(X[, 1], Y[, 1], pch = 16, main = &quot;NN model after 11000 iterations&quot;)
lines(xt[, 1], yt[, 1])</code></pre>
<p><img src="/post/neural_networks/02methods_nn_rcode_files/figure-html/unnamed-chunk-26-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="conclusions" class="section level2">
<h2>Conclusions</h2>
<p>Even this simple example raises many questions.</p>
<ul>
<li>why a (1, 4, 1) architecture?<br />
</li>
<li>how would I tell that 1000 iterations was not enough if it were not possible to plot the data, say because there were multiple x’s or multiple y’s?<br />
</li>
<li>what would happen if I ran 100,000 iterations?</li>
<li>why a step size (learning date) of 0.1? Would other step sizes work better?<br />
</li>
<li>could the step size be adjusted as the algorithm progresses?<br />
</li>
<li>would alternative activation functions produce a better model?<br />
</li>
<li>how critical are the random starting values?<br />
</li>
<li>do all starting values lead to the same final NN?<br />
</li>
<li>are there different sets of parameters that produce more or less the same loss?<br />
</li>
<li>is the successful performance, specific to this particular random set of 15 observations?<br />
</li>
<li>would the NN still find the quadratic if the data had a larger random component?<br />
</li>
<li>could a small change in the training data cause the NN to choose a very different response curve?<br />
</li>
<li>would convergence be improved if I scaled the training data?<br />
</li>
<li>are there any clues in the data that would help me choose the architecture of the network?</li>
</ul>
<p>I do what to investigate these and other questions, but first I will speed up the computation by converting my R code to C++. That will be the topic for my next post.</p>
</div>
</div>
