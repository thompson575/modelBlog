---
title: "Bayesian Sliced 4: Rain Tomorrow"
author: "John Thompson"
date: "2022-10-15"
layout: post
tags:
- Sliced
- hierarchical model
- mixed logistic regression  
- Stan
- brms
- lme4 package
- missing values in a Bayesian analysis
output:
    html_document:
    keep_md: true
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE, fig.align='center', fig.height=4, fig.width=5.5)
library(targets)
archive <- "C:/projects/sliced/Bayes-s01-e04/_targets"
```
# Summary:

**Background:** In episode 4 of the 2021 series of *Sliced*, the competitors were given two hours to analyse a set of data on rainy days in Australia. The aim was to use today's weather data to predict whether or not it will rain tomorrow.    
**My approach:** In this post, I use a series of hierarchical models, where the location within Australia is one level and day within the location is the other. I fit the models by both likelihood and Bayesian methods. I finish by running a Bayesian analysis across different imputed datasets in order to adjust for the uncertainty introduced by missing values.         
**Result:** Bayesian models fitted with `stan` give very similar estimates to models fitted by maximum likelihood with `glmer`, although the Bayesian models are more robust and are less likely to run into convergence issues.     
**Conclusion:** Hierarchical models are a natural option for weather data on days within locations. However, this dataset is large with many days of weather data for each city, so it is possible to create a good within city model without the need to learn from the experience of other cities. For big data problems, the gains from hierarchical modelling are limited.

# Introduction

The fourth of the *Sliced* tasks is to predict whether or not it will rain tomorrow based on weather records for 49 different locations in Australia. The data can be downloaded from https://www.kaggle.com/c/sliced-s01e04-knyna9.. 

In my original post, I analysed the 49 locations separately and used the appropriate model to make my predictions. I noted at the time that a hierarchical model would have been preferable, but I resisted the temptation to fit such a model, because I knew that it would be computationally demanding and I had already spent a lot of time on the data cleaning and the imputation of missing values.

In this post, I investigate the use of hierarchical models for these data. I start by fitting the models by maximum likelihood; this requires numerical integration and the algorithm can be numerically unstable when the number of parameters is large, or when the random effects have small variances. Then, I fit equivalent Bayesian hierarchical models in `stan`. I use the `brms` package to prepare the `stan` code.

I adjust the final model for the uncertainty due to the missing values by repeating the Bayesian analysis on each of five imputed datasets.

# Reading the data

In the original post, I cleaned the data and, because of the number of missing values, I used the `mice` package to impute five sets of complete data. I pick up the analysis by reading the first of those multiple imputations.
```{r}
# --- setup: libraries & options ------------------------
library(tidyverse)
library(mice)

theme_set( theme_light())

# --- set home directory -------------------------------
oldHome <- "C:/Projects/sliced/s01-e04"
home    <- "C:/Projects/sliced/Bayes-s01-e04"

# --- read results from mice ---------------------------
imp2 <- readRDS(file.path(oldHome, "data/rData/imp2.rds"))

# --- extract 1st set of imputed training data ---------
complete(imp2, action=1) %>%
  slice( 1:34191 ) %>%
  as_tibble() -> trainDF

# --- select 15000 observations for estimation ----------
set.seed(8231)
split <- sample(1:34191, size=15000, replace=FALSE)

# --- extract training data and scale ------------------
trainDF %>%
  slice(split) %>%
    mutate( across( contains("3pm"), scale),
            wind_gust_speed = scale(wind_gust_speed)) -> estimateDF
```

I deliberately make the estimation set relatively small (n = 15,000) in order than the model development is speeded up. Hierarchical models are slow to fit, whatever algorithm you use.  

Convergence of both likelihood and Bayesian analyses is improved when the explanatory variables are measured over similar ranges, so I scale the continuous variables that I plan to use in my models.

# The need for a hierarchical structure

In my previous post, I analysed each location separately, for example, here is a typical model fitted to the data from Perth in Western Australia. The offset `logitBayes` is the logit of the probability that it will rain on any given day during that month in that location as estimated from the training data. It acts as a baseline when estimating the logit of the probability of `rain_tomorrow`.

```{r }
library(broom)
# --- glm for Perth -------------------------------------------
estimateDF %>%
  filter( location == "Perth") %>%
  glm( rain_tomorrow ~ rain_today + humidity3pm +
           temp3pm + pressure3pm + wind_speed3pm, 
           family="binomial", offset=logitBayes, data=.) %>%
  tidy()
```

Not surprisingly, high humidity is associated with an increased chance of rain the following day. Analysing all 49 locations (not shown) indicates that the sizes of these associations do vary from location to location, but the broad pattern is the same everywhere. 

It makes sense to think of an average humidity effect for all Australia with individual cities varying randomly about that average. These variations will follow a zero-centred distribution and by building that distribution into the analysis, the estimate of the humidity effect for Perth will be influenced by the humidity effects in other locations, hopefully making Perth's estimate more accurate and more numerically stable.  

In locations where the humidity effect is either unusually large or unusually small, the random effect will move that estimate towards the centre of the distribution, that is, towards Australia's average effect size. This phenomenon, usually called shrinkage, ought to protect against extreme predictions based on little data.

# Likelihood analysis with `glmer()`

The hierarchical structure leads us to a so-called mixed model; mixed because if includes both a fixed effect and a random effect. The fixed effect is the average across all of Australia and the random effect is the variation between locations.

The function `lmer()` from the package `lmer` fits mixed models when the response is normally distributed. The calculation of the likelihood requires integration over the random effects, which are modelled as being zero-centred normal distributions. The joint normality means that the likelihood can be integrated analytically, greatly simplifying the algorithm.

When the response is not normally distributed, mixed models are fitted with the `glmer()` function, also from the `lmer` package; it is an extension of the `glm()` function. Once again, the random effects are modelled as being zero-centred normal distributions, but now the integral is intractable and it needs to be approximated numerically. 

The `glmer` algorithm maximises a likelihood that, at each stage, is evaluated by numerical integration. The algorithm is slow and can be unstable. Stability is improved by adding more points to the numerical integration, but then the algorithm becomes even slower. 

As well as the fixed effects, such as the average humidity effect for all of Australia, the algorithm estimates the variances of the zero-centred random effects. In situations where there is not much variation between locations, such variances will be small and the algorithm has to search for a solution that lies on, or close to, the edge of the parameter space. This is notoriously difficult to do in a numerically stable way.  

## A model with a random intercept

The first model is a logistic regression for the binary response `rain_tomorrow` (Yes=1, No=0) with three predictors and an offset.

Without a hierarchical structure this is just a generalized linear model that can be fitted by `glm()`.
```{r eval=FALSE}
# --- Model 1: fitted by glm ----------------------------------
glm(rain_tomorrow ~ 1 + rain_today + humidity3pm + pressure3pm, 
              family=binomial(link=logit), 
              offset = logitBayes,
              data = estimateDF) %>%
  { tidy(.) %>% 
      print()
    glance(.) %>% 
      print()}
```

```{r echo=FALSE}
tar_read(glmFit_02, store=archive) %>%
  { tidy(.) %>% 
      print()
    glance(.) %>% 
      print()}
```

The intercept measures the deviation from the offset when the other predictors are zero, i.e. no rain_today, national average humidity and pressure. This intercept can be allowed to vary by location by adding a random effect.

```{r eval=FALSE}
# --- Model 1: mixed model fitted by glmer --------------------
library(lme4)

glmer(rain_tomorrow ~ 1 + rain_today + humidity3pm + pressure3pm + 
        ( 1 | location), 
              family=binomial(link="logit"), 
              offset = logitBayes,
              data = estimateDF) %>%
  summary()
```

```{r echo=FALSE}
tar_read(glmerFit_02, store=archive) %>%
  summary()
```

The fixed effects are similar to those of the glm() model, but the extra
random term, (1 | location), allows the intercepts to vary by location.  Over Australia as a whole, the intercept is 0.626. However, this intercept is not uniform across the country; the intercepts over the 49 locations have an estimated variance of 0.365.   

## A model with random coefficients

The previous model assumed that the impacts of the predictors, rain_today, humidity and pressure, are the same everywhere. So, although the intercept, which represents the chance of rain tomorrow when there is no rain today, is allowed to vary with location, the difference between no rain today and some rain today is assumed to be the same everywhere.

The next model allows the coefficients of the predictors to vary as well as the intercept.
```{r eval=FALSE}
# --- Model 2: mixed model fitted by glmer --------------------

glmer(rain_tomorrow ~ 1 + rain_today + humidity3pm + pressure3pm + 
        ( 1  + rain_today + humidity3pm + pressure3pm | location), 
              family=binomial(link="logit"), 
              offset = logitBayes,
              data = estimateDF) %>%
  summary()
```

```{r echo=FALSE}
tar_read(glmerFit_04, store=archive) %>%
  summary()
```

The average impact of each of these predictors is significant and the effect sizes vary with location. The variances of the random effects indicate that there is wide variation in the intercepts and pressure coefficients, but less variation in the coefficient of humidity and rain today.

The default for `glmer` is to assume that all of the random effects are correlated and the results table gives the estimates of those correlations. For instance, the intercept and rain_today have an estimated correlation of -0.89. So, in places where the intercept is high, the effect of rain today tends to be smaller.

To illustrate the range of models that `glmer()` can fit, I try a slight variation on this model. I remove the random effect on rain today that is, I assume that rain today has the same effect everywhere. Then, I assume that the random effect on the intercept and random effects on the other predictors are independent.

```{r eval=FALSE}
# --- MODEL 3: simplified glmer model ------------------------
estimateDF %>%
  glmer(rain_tomorrow ~ 1 + rain_today + humidity3pm + pressure3pm +
              (1 | location) +  
              (0 + humidity3pm + pressure3pm | location), 
              offset = logitBayes,
              family=binomial(link=logit), 
              data = .) %>%
  summary()
```

```{r echo=FALSE}
tar_read(glmerFit_06, store=archive) %>%
  summary()
```

Notice that the fixed effects sizes are very similar to those in the model with the more general correlation structure. Removing the correlation makes the computation quicker and more stable, but the model may be a little unrealistic.

# Bayesian mixed models with `stan`

The main differences between likelihood and Bayesian mixed models are the priors and fitting algorithm. The priors, especially those on the variances, should improve convergence and, as the Bayesian algorithm uses simulation, it does not require numerical integration. The combined effect is that the Bayesian algorithm should be more stable. However, a Bayesian analysis needs to simulate parameters for every location and the Bayesian algorithm will be slower.

I have a choice, I could either write my own `stan` code, or I could use `brms` or `stanarm`. These packages contain functions that write the `stan` code for you. I have opted to use `brms`.

I start with the `brms` equivalent of model 2.

```{r eval=FALSE}
# --- Model 2 fitted with stan via brms -------------------
library(brms)

estimateDF %>%
  brm(rain_tomorrow ~ 1 + rain_today + humidity3pm + pressure3pm +
              offset(logitBayes) +
              (1 + rain_today + humidity3pm + pressure3pm | location), 
              family=bernoulli(link=logit), 
              data = .,
              prior = c(prior(normal(0, 1), class = Intercept),
              prior(normal(0, 1), class = b),
              prior(normal(0, 1), class = sd)),
      iter = 2000, warmup = 1000, thin = 1, 
      chains = 4, cores = 4, seed = 3491) %>%
  summary()
```

```{r echo=FALSE}
tar_read(bay04, store=archive) %>%
  summary()
```

The fixed effect estimates are almost identical to those given by `glmer`. `brm()` reports standard deviations rather than variances, but once again they are almost identical to those of `glmer`, although the correlation is a little smaller, perhaps because of the independent priors. In this case `glmer` and `stan` are effectively equivalent.

# A more complex model

In my previous post on these data, I got reasonable results with a location-specific model based on rain_today, humidity, pressure, temperature and wind speed in the afternoon and the wind gust speed. Here is the `glmer` version of the equivalent mixed model.

```{r eval=FALSE}
# --- model 4: more complex mixed model -------------------------------
estimateDF %>%
  glmer(rain_tomorrow ~ 1 + rain_today + 
                humidity3pm + pressure3pm + temp3pm +
                wind_speed3pm + wind_gust_speed +
                (1 + rain_today + 
                humidity3pm + pressure3pm + temp3pm +
                wind_speed3pm + wind_gust_speed | location),
                offset = logitBayes,
                family=binomial(link=logit), 
                data = .) %>%
  summary()
```

```{r echo=FALSE}
tar_read(glmerFit_07, store=archive) %>%
  summary()
```

**Notice that the algorithm failed to converge**

Although `glmer` did not converge, the solution is probably roughly OK. It suggests that temperature and rain_today are of minor importance. Wind speeds are important, but the coefficients don't vary much from place to place.

Here is the Bayesian version of the same mixed model.
```{r eval=FALSE}
# --- model 4 fitted by stan -------------------------------------
estimateDF %>%
  brm(rain_tomorrow ~ 1 + rain_today + 
                humidity3pm + pressure3pm + temp3pm +
                wind_speed3pm + wind_gust_speed +
                offset(logitBayes) +
                (1 + rain_today + 
                humidity3pm + pressure3pm + temp3pm +
                wind_speed3pm + wind_gust_speed | location),
              family=bernoulli(link=logit), 
              data = .,
              prior = c(prior(normal(0, 1), class = Intercept),
              prior(normal(0, 1), class = b),
              prior(cauchy(0, 1), class = sd)),
      iter = 2000, warmup = 1000, thin = 1, 
      chains = 4, cores = 4, seed = 3491) %>%
  summary()
```

```{r echo=FALSE}
tar_read(bay07, store=archive) %>%
  summary()
```

The estimates are very similar to those from the `glmer` analysis that failed to converge.

The Rhat values for the estimates in the Bayesian analysis are all close to 1, suggesting good convergence. Further convergence checks are needed, but I'll not show them here. The fixed effect (called population-level effect, which is a better name) of temperature has a 95% credible interval that includes zero. 

# Missing values

Handling missing values in a Bayesian analysis is theoretically straightforward, but computationally challenging.

The missing values could be estimated as part of the HMC or MCMC chain, by sampling from the conditional distribution given the observed data and the current parameter estimates. 
\[
x_{miss} \sim p(x_{miss} | x{obs}, y, \theta)
\]
Effectively, each missing value is treated as if it were an extra parameter in a Gibbs sampler. When there are many missing values, this computation becomes impractical.

A multiple imputation package such as `mice` performs much the same calculation, except

* the imputation is performed 5 or 10 times, rather that at every iteration of the HMC or MCMC algorithm   
* the model used for imputation is usually a chained regression rather than actual model used in the Bayesian analysis   

If we are willing to overlook these differences, the full Bayesian missing value analysis can be approximated by fitting the analysis model to each of the imputed data sets and then pooling the resulting simulations.

The function `brm_multiple` automates this process and works directly on the object return by `mice`, but I opted to fit the same model manually to each of my 5 imputed data sets.

I ran the computations for the whole of this post with the `targets` package, as described in my post **Creating a blog post with targets**. The code below stacks the 5 sets of simulations produced by these analyses, adds a variable that I call `action` to denote the 5 imputed datasets and drops the warmup.
```{r}
# --- read results from the targets archive -----------------
tar_read(mvFit_01, store = archive) %>%
  mutate( action = 1) %>%
  bind_rows( tar_read(mvFit_02, store = archive) %>%
               mutate( action = 2)) %>%
  bind_rows( tar_read(mvFit_03, store = archive) %>%
               mutate( action = 3)) %>%
  bind_rows( tar_read(mvFit_04, store = archive) %>%
               mutate( action = 4)) %>%
  bind_rows( tar_read(mvFit_05, store = archive) %>%
               mutate( action = 5)) %>%
  filter( iter > 1000 ) %>%
  mutate( action = factor(action) ) -> simDF
```

For illustration, I'll take the coefficient of the fixed effect of `rain_today` and the variance of the corresponding random effect. Here are the posterior means and standard deviations, first by action and then overall.

```{r}
# --- posterior mean and std by action ---------------------
simDF %>%
  group_by( action ) %>%
  summarise( m   = mean(b_rain_today1),
             s   = sd(b_rain_today1),
             mre = mean(sd_location__rain_today1^2),
             sre = sd(sd_location__rain_today1^2) )

# --- pooled posterior mean and std ------------------------
simDF %>%
  summarise( m = mean(b_rain_today1),
             s = sd(b_rain_today1),
             mre = mean(sd_location__rain_today1^2),
             sre = sd(sd_location__rain_today1^2))
```

Here are the distributions of the variance of the random effect plotted separately for each imputed dataset.
```{r}
simDF %>%
  mutate( action = factor(action)) %>%
  ggplot( aes(x=sd_location__rain_today1^2, fill=action)) +
  geom_density( alpha=0.3) +
  labs( x = "Variance of rain today random effect")
```

The small differences in the distributions are due in part to the missing values and in part to the MC error of the HMC algorithm. I only ran 2 chains of 1000 post warmup samples on each imputed dataset; longer runs would reduce the MC error and make the effect of the missing values more evident.


# Conclusions

Fitting Bayesian hierarchical models is not quick, but the priors have the effect of making the models more stable and they help avoid the convergence failures that are common in likelihood analyses.  

Although I have skipped the detail in this post, it is vitally important to inspect the chains produced by a Bayesian analysis to assure yourself that the algorithm really has converged. Failure of a likelihood analysis is fairly clear cut, but failure of a Bayesian analysis is much harder to detect.  

A simple mixed model assumes that the 49 cities provide independent samples of the weather in Australia, which of course they do not. The weather in Newcastle is much more similar to the weather in neighbouring Sydney than it is to the weather in Darwin or Perth. A better model would incorporate the geography of the country. One way to do this would be to introduce a state variable, so that the hierarchical structure would be day within location within state within country. Australian states are very large, so this approach would not be ideal, but it is simple and it would probably help.

The big gains from having a hierarchical structure are seen when the data on some of the levels are sparse. Imagine, adding a new location with only one week of weather data. The estimates for that locations would be unreliable, but they would be improved by making use of the pattern of data across the other cities. However, when the data are not sparse, very little transfer of information takes place and the hierarchical estimates will be similar to those that would have been obtained from an independent analysis of that city. In this data set of 15,000 observations over 49 locations, there is an average of over 300 days' data for each location. The within location parameter estimates are well-identified and the extra computation required for a hierarchical model is probably not justified.

