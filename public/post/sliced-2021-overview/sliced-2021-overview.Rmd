---
title: "Sliced 2021 Overview"
author: "John Thompson"
date: "`r Sys.time()`"
layout: post
tags:
- Sliced
- Episode overview
output:
    html_document:
    keep_md: true
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

Sliced is a data science competition hosted by Nick Wan on twitch ( https://www.twitch.tv/nickwan_datasci ). In each episode, 4 data scientists with very different backgrounds are given the same dataset and in 2 hours they must explore the data and build a predictive model. As well as the training data, they are given a test set that lacks the variable to be predicted. The model that best predicts in the test data according to a specified metric, wins the prediction element of the competition, but other points are available for data visualization, golden features and popularity as voted for by chat.

The datasets are all available on kaggle and while the competition is live on twitch, members of the audience can run their own analyses and submit them so that they are assessed alongside the models of the competitors. Afterwards, anyone can download the data and submit a late entry, but it will not be added to the leaderboard.

I have taken each of the datasets used by sliced and run my own analyses, which I present in a series of blog posts. I think that the main interest in my analyses lies in the fact that I was trained as a statistician and not as a data scientist, so my analyses highlight some interesting differences of approach between the two traditions.  

I must stress that I did not analyse these data under competition conditions. In most cases, I spend less than 2 hours on the analysis, but much more time writing the blog posts that explain what I did.

In this post, I give an overview of each episode and summarise the methods that I used. If you are looking for examples of a particular form of analysis, this will show you which posts to go to.

# My approach

I analyse all of the datasets in R, but prefer not to use `tidymodels`. The tidymodels package forces you into a machine learning mindset and it is my belief that this approach has several important weaknesses that lead to inferior results. Exposing those weaknesses is one of my main reasons for writing this blog. 

I do make heavy use of the `tidyverse` because it produces quick and readable code. I supplement this with my own functions and a smattering of base R. For a couple of the later episodes I have used `mlr3` a machine learning competitor of `tidymodels` that I have been interested in for some time, but not used before. 

In my opinion statistics and machine learning are closely related and the key differences are not in the models that they use, but in the ways in which they use them. So, I am quite happy to use tree-based model including xgboost. What I find unsatisfactory is when tree-based model are used in an automated pipeline. I'll discuss points such as these at the end of each post.

Here are some differences of approach between my analyses and the machine learning analyses favoured by most of the competitors

* an emphasis on understanding what the data mean and how they were collected  
* data cleaning as a separate step carried out prior to modelling  
* a liking for moving from the simple to the complex  
* a preference for simple models  
* a preference for models that can be interpreted  
* a preference for models that would generalise  
* a hatred of black box methods  
* a hatred of long pipelines in which the analyst does not look at the intermediate results  
* emphasis on model checking and model interpretation    
* scepticism about obsessional hyperparameter tuning  
* scepticism about meaningless improvement is prediction accuracy (not a good attitude to have in a data science competition)  

For each episode, I have tried to find a different approach so that the posts are not repetitive. Occasionally, the search for variety is at the expense of predictive performance, but on the whole my models are competitive with the submission made live during the competition and some of them would have won. Given that I was not under time pressure this is perhaps not surprising.  

# The episodes

## Episode 1: Boardgames

### Keywords

boardgame rating; transformed response; text features; user written functions; replacing impossible values; generalized additive models; splines; interactions;

### Packages

tidyverse; broom; mgcv

### The data

The data can be download from https://www.kaggle.com/c/sliced-s01e01.  

### The problem

The data were taken from the website https://boardgamegeek.com/. The training set has full details on 3499 boardgames. The object is to predict the geek rating for each game. Some predictors are numeric, such as number of players, time to play the game and the year that the game was released but others are textual, such as a description of the mechanics of the game. 

The test data has the same predictor variables on 1500 games, but has the ratings for those games removed. Predictive models are assessed using the root mean square error (RMSE).

### My Analysis

My exploratory analysis led me to work with a transformed response, log10(geek_rating-5.5). The models predicted this transformed response and then I back-transformed to get the predicted geek rating.  

Data cleaning was minimal because the data are reasonably complete, with just a few missing values and a handful of extreme or unlikely looking values.

My analysis of the textual data is fairly basic. I extract 10 features relating to the game mechanics and 10 features related to the category of the game. To do this I wrote simple functions that are included as a appendix to the post.

There is quite a degree of non-linearity in the relationships between geek rating and the predictors, so I opted for splines in a GAM (generalized additive model). In R, GAMs can be fitted with the `gam` package, but I prefer a package called `mgcv` because it incorporates methods for estimating the degree of smoothness that is required. It also allows 2-dimensional splines that represent interactions.

My final model did well and have come close to the top of the leaderboard.  

In the post, I try to show that predictions from linear models can be competitive (you do not always need to used XGBoost) and at the same time, the models are interpretable. I also emphasise that it is not necessary, or even advisable, to make all of your modelling decisions based on minimising the loss function.

## Episode 2: Wildlife strikes

### Keywords

wildlife strikes; data cleaning; missing data; imputation; categorising text features; user-written ggplot function; curly-curly {{}}; nonstandard evaluation; logistic regression; analysis of deviance; Hosmer-Leweshow plot;

### Packages

tidyverse; broom; forcats; lubridate;

### The data

The data can be download from https://www.kaggle.com/c/sliced-s01e02-xunyc5.

### The problem

These data were collect by the Federal Aviation Authority (FAA) in the USA and contain details of wildlife strikes with aircraft. Everything from a commercial jet running into a flock of sparrows to a private plane hitting an elk. The objective is to predict whether or not the aircraft was damaged in the collision. The metric used for evaluation was the mean logloss.

As you might expect, these data are far from perfect; lots of missing information and factors with hundreds of levels.

### My Analysis

This is primarily an exercise in data cleaning. I have a great deal of sympathy with the competitors who would have been under real pressure to cut short the cleaning and jump to the model fitting. It was a tough dataset to use in this type of competition.

Most of my code performs data cleaning and data exploration. Once the data were in a good form for analysis, I used a simple logistic regression and got perfectly good results.

For the data exploration, I wrote a function that creates a stacked bar chart with added annotation to give the percent of aircraft damaged. The code uses curly-curly {{}} to provide nonstandard evaluation (NSE), which allows the function to be incorporated into a pipe.  

I decided against imputation of the missing data, because the exploration made it clear that the data were not missing at random. Instead, I added a missing category to each of the categorical variables and used missingness as a predictive feature.

I used an analysis of deviance table to look at contributions to the logistic regression model and plots based on the Hosmer-Leweshow statistic to assess the fit. I calculated a cross-validation estimate of model performance, but noted that, in this type of example, the in-sample estimate is perfectly adequate.  


## Episode 3: Superstore profits

### Keywords

superstore profits; knowledge external to the data; linear models; offsets; weaknesses of machine learning;

### Packages

tidyverse; broom;

### The data

The data for this episode are available from https://www.kaggle.com/c/sliced-s01e03-DcSXes.

### The problem

The dataset contains information on products sold by an on-line store in the USA and the objective is to predict the profit made on each item. Evaluation is by RMSE.  

Compared with the previous episode, these data are a pleasure to analyse. There is very little processing required before we start modelling and the structure is simple. 

The most important aspect of this problem is that we have data on profits made on individual items but the items are only categorised into very broad groups, such as tables or copiers. Obviously, some tables cost more than others and more importantly, some are sold at a discount. We are told the price of the individual items and the size of any discount that was applied.  

### My Analysis

There is a class of modelling problems were we know something about the relationship between the variables. The classic example is a physics experiment, where we know that all of the variables must obey the natural laws of physics. In such cases, it is vital that the model takes these known laws into account, otherwise it is unlikely to perform well. Analysts who jump in with their favourite algorithm are likely to do poorly on this type of problem.

In the case of the superstore, we know the relationship between sales price, discount and profit. If you use this known relationship then 

* it tells us what structure the model should have  
* it makes it obvious that most of the predictors can be ignored  
* it produces extremely good results  

My model beats the submissions on the leaderboard by a wide margin, so I can only assume that those competitors ignored the specific structure of the problem.  

The code used for this problem is very basic. All that you needed to win the competition were a few scatter plots and some simple linear models.
